Criando um Monitoramento de Custos no Data Factory


üöÄ Vis√£o Geral
Este projeto apresenta uma abordagem pr√°tica para explorar o ambiente Azure utilizando uma conta gratuita de estudante. O objetivo principal √© configurar o Azure Data Factory e monitorar o uso e os custos dos recursos implantados. Durante a implementa√ß√£o, abordaremos os seguintes temas:

‚úîÔ∏è Estrutura√ß√£o de assinaturas e grupos de recursos.
‚úîÔ∏è Boas pr√°ticas de nomenclatura para facilitar a organiza√ß√£o.
‚úîÔ∏è Personaliza√ß√£o de dashboards para acompanhamento visual.
‚úîÔ∏è Utiliza√ß√£o de m√©tricas e alertas de custo para controle eficiente.
‚úîÔ∏è Automa√ß√£o com ARM Templates e Azure Cloud Shell.
‚úîÔ∏è Passo a passo completo para monitorar os gastos na plataforma.
‚úîÔ∏è Implementa√ß√£o de um script Python para automa√ß√£o do monitoramento de custos.
üõ†Ô∏è Tecnologias Utilizadas
Azure Data Factory üåê
Azure Monitor üìä
Azure Cost Management üí∞
Azure Cloud Shell üñ•Ô∏è
ARM Templates üìÑ
Python + Azure SDK üêç
üìå Pr√©-requisitos
Antes de iniciar, certifique-se de ter:

üîπ Uma conta gratuita no Azure for Students ou Azure Free Tier.
üîπ Acesso ao Azure Portal.
üîπ Familiaridade com o Azure Resource Manager (ARM).
üîπ Conhecimentos b√°sicos em PowerShell, CLI do Azure e Python.


üèó Passo a Passo da Implementa√ß√£o de um projeto de monitoramento de custos



1Ô∏è‚É£ Criando o Ambiente no Azure
Acesse o portal Azure Portal.
Crie um Grupo de Recursos para organizar os ativos.
Configure uma Assinatura vinculada √† conta gratuita.
Defina um Plano de Nomenclatura para manter a padroniza√ß√£o.



2Ô∏è‚É£ Implantando o Azure Data Factory
No Azure Portal, v√° at√© "Criar um recurso" e selecione Data Factory.
Escolha o grupo de recursos criado anteriormente.
Defina a regi√£o e nome do servi√ßo seguindo as boas pr√°ticas.
Conclua a implanta√ß√£o e acesse o painel do Data Factory.
3Ô∏è‚É£ Monitorando os Custos
Acesse Azure Cost Management + Billing no portal.
Configure m√©tricas de uso e custo para o Data Factory.
Defina alertas de custo para evitar gastos inesperados.
Crie um dashboard personalizado para visualiza√ß√£o simplificada.




4Ô∏è‚É£ Automa√ß√£o com Azure Cloud Shell
Acesse o Azure Cloud Shell no portal.
Utilize PowerShell ou Azure CLI para gerenciar recursos.
Automatize tarefas recorrentes com scripts ARM Templates.
Teste a cria√ß√£o e remo√ß√£o automatizada de recursos para otimiza√ß√£o.
5Ô∏è‚É£ Implementa√ß√£o com Python üöÄ
Podemos automatizar o monitoramento dos custos do Azure Data Factory utilizando Python e a Azure SDK. Siga os passos abaixo:

Instale as bibliotecas necess√°rias:
pip install azure-identity azure-mgmt-costmanagement

C√≥digo em Python para monitoramento de Custos:
Abra o arquivo monitoramento de custos.py 


Esse script autentica no Azure, consulta os custos do Data Factory e gera alertas caso os gastos ultrapassem o limite definido. üöÄ

üéØ Resultados Esperados
‚úÖ Melhor controle dos custos no Azure üí∞.
‚úÖ Configura√ß√£o eficiente do Data Factory üèó.
‚úÖ Dashboards intuitivos para acompanhamento üìä.
‚úÖ Automa√ß√£o e infraestrutura como c√≥digo üìú.
‚úÖ Monitoramento automatizado via Python ü§ñ.



Redund√¢ncia de Arquivos no Azure com Data Factory
‚ú® Vis√£o Geral
Este projeto tem como objetivo criar um processo completo de redund√¢ncia de arquivos utilizando recursos do Microsoft Azure. Atrav√©s do Azure Data Factory, voc√™ aprender√° a configurar toda a infraestrutura necess√°ria para mover dados entre ambientes on-premises e a nuvem, garantindo backup seguro e acess√≠vel.

üîÑ Fluxo do Processo
Conectar fontes de dados: SQL Server local e Azure SQL Database.
Criar Linked Services: Estabelecendo conex√£o entre o Data Factory e os reposit√≥rios de dados.
Criar Datasets: Defini√ß√£o das estruturas para entrada e sa√≠da dos dados.
Criar Pipelines: Constru√ß√£o dos fluxos de trabalho para movimenta√ß√£o dos dados.
Converter e armazenar: Transformar dados em arquivos .TXT, organizando-os em camadas (raw/bronze) dentro do Azure Data Lake.
Publicar e Executar: Valida√ß√£o e execu√ß√£o do pipeline.
Analisar performance: Aplicando boas pr√°ticas para otimizar o processo.
üõ†Ô∏è Tecnologias Utilizadas
Microsoft Azure (üåç Plataforma Cloud)
Azure Data Factory (üõ† Orquestra√ß√£o de Dados)
SQL Server (On-Premises e Azure SQL Database) (üíæ Banco de Dados Relacional)
Azure Blob Storage (üè¢ Armazenamento de Arquivos)
Integration Runtime (‚ö° Conectividade H√≠brida)
üóíÔ∏è Passo a Passo da Implementa√ß√£o
1. Criando o Azure Data Factory
Acesse o portal do Azure.
Crie um novo Data Factory.
Configure o Integration Runtime para conectar-se ao SQL Server local.
2. Configurando os Linked Services
Adicione um Linked Service para o SQL Server On-Premises.
Adicione um Linked Service para o Azure SQL Database.
Adicione um Linked Service para o Blob Storage.
3. Criando os Datasets
Crie um Dataset apontando para a tabela SQL de origem.
Crie um Dataset para os arquivos .TXT de destino no Blob Storage.
4. Criando o Pipeline
Adicione um Copy Activity para mover os dados do SQL Server para o Azure Data Lake.
Configure a transforma√ß√£o dos dados em arquivos .TXT organizados por camadas (raw/bronze).
Teste e valide as transfer√™ncias.
5. Publicando e Executando
Publique as altera√ß√µes e execute o pipeline.
Monitore os logs e valide a performance.
üí° Utilizando o SDK do Azure no Python
O SDK do Azure para Python permite interagir programaticamente com os servi√ßos da nuvem, como o Azure Blob Storage. Com ele, voc√™ pode realizar opera√ß√µes como upload, download e gerenciamento de arquivos de forma eficiente.

Para instalar:

pip install azure-storage-blob

Exemplo de c√≥digo para upload de arquivo no Azure Blob Storage abra o arquivo:
  projeto redund√¢ncia de arquivos.py



Confira a documenta√ß√£o oficial:  (https://learn.microsoft.com/en-us/azure/storage/blobs/)
   para mais detalhes sobre suas funcionalidades.



Benef√≠cios
üåê Alta disponibilidade: Backup autom√°tico na nuvem.
‚öñÔ∏è Seguran√ßa: Dados armazenados de forma redundante.
‚è≥ Efici√™ncia: Pipelines otimizados para melhor desempenho.


Azure Databricks - Versionamento e Organiza√ß√£o de Notebooks
Descri√ß√£o
Este projeto demonstra como utilizar o Azure Databricks para versionamento e organiza√ß√£o de notebooks em ambientes de dados. A proposta inclui:

Cria√ß√£o e configura√ß√£o de clusters;
Importa√ß√£o e execu√ß√£o de notebooks com suporte de Intelig√™ncia Artificial;
Integra√ß√£o com Azure DevOps para controle de c√≥digo e automa√ß√£o de pipelines de CI/CD;
Uso da IA integrada ao Databricks para gera√ß√£o de c√≥digo Python e Spark;
Boas pr√°ticas para organiza√ß√£o, exporta√ß√£o e reaproveitamento de notebooks;
Explora√ß√£o de recursos do Microsoft Learn, com exerc√≠cios guiados e roteiros de aprendizado;
Trabalho colaborativo e seguro com versionamento estruturado em engenharia de dados e machine learning.
Arquitetura
A arquitetura deste projeto segue o fluxo abaixo:

Cria√ß√£o de um Cluster no Azure Databricks.
Importa√ß√£o de arquivos e notebooks para execu√ß√£o.
Execu√ß√£o de notebooks interativos com filtros, sumariza√ß√µes e visualiza√ß√µes.
Gera√ß√£o de c√≥digo com suporte de IA integrada ao Databricks.
Integra√ß√£o com Azure DevOps para versionamento e CI/CD.
Automa√ß√£o de pipelines para controle das execu√ß√µes e governan√ßa.



Tecnologias Utilizadas
Azure Databricks
Python
Apache Spark
Azure DevOps
Microsoft Learn
CI/CD (Continuous Integration & Continuous Deployment)
MLflow (para versionamento de modelos em ML)
Passo a Passo - Configura√ß√£o do Ambiente
1. Criar um Cluster no Databricks
Acesse Azure Databricks
Navegue at√© Clusters > Create Cluster
Escolha um nome e selecione a configura√ß√£o de hardware necess√°ria
Clique em Create Cluster
2. Importar e Executar um Notebook
No Databricks, acesse Workspace
Clique em Import e carregue um arquivo .ipynb ou .dbc
Abra o notebook para edi√ß√£o e execu√ß√£o
3. Configurar Azure DevOps para Versionamento
No Azure DevOps, crie um reposit√≥rio Git
Conecte o Databricks ao Azure DevOps:
V√° para Repos > Git Integration
Configure a conex√£o ao seu reposit√≥rio remoto
Habilite CI/CD Pipelines para automa√ß√£o
4. Criar um Pipeline CI/CD no Azure DevOps
No Azure DevOps, v√° para Pipelines > New Pipeline
Escolha GitHub ou Azure Repos Git como origem do c√≥digo
Selecione Starter Pipeline e edite o azure-pipelines.yml
Adicione o seguinte c√≥digo para execu√ß√£o automatizada de notebooks:

trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: DatabricksRunNotebook@0
  inputs:
    databricksServiceConnection: 'AzureDatabricks'
    notebookPath: '/Workspace/MeuNotebook'
    workspaceUrl: 'https://adb-123456789.azuredatabricks.net'
    newCluster:
      clusterName: 'ci-cd-cluster'
      nodeTypeId: 'Standard_DS3_v2'
      sparkVersion: '7.3.x-scala2.12'


Salve e execute o pipeline para testar a automa√ß√£o

C√≥digo em Python e Spark

 c√≥digo para leitura, transforma√ß√£o e exibi√ß√£o de dados em um notebook do Databricks: abra o arquivo: Projeto controle e versionamento de c√≥digo pyspark


Benef√≠cios do Projeto
‚úÖ Melhor organiza√ß√£o e versionamento de notebooks ‚úÖ Automa√ß√£o de processos com CI/CD ‚úÖ Colabora√ß√£o eficiente em times de engenharia e ci√™ncia de dados ‚úÖ Uso de Intelig√™ncia Artificial para facilitar o desenvolvimento ‚úÖ Governan√ßa e seguran√ßa no controle de c√≥digo


Integrando o Azure Data Factory ao Azure DevOps
Vis√£o Geral
Este projeto demonstra como integrar o Azure Data Factory ao Azure DevOps, permitindo versionamento, controle de mudan√ßas e backups autom√°ticos de pipelines e artefatos de dados. Essa integra√ß√£o assegura maior governan√ßa e rastreabilidade no desenvolvimento de solu√ß√µes de dados.

Benef√≠cios da Integra√ß√£o
Versionamento de Pipelines: Manuten√ß√£o de hist√≥rico de altera√ß√µes.
Controle de Mudan√ßas: Padroniza√ß√£o e rastreamento de ajustes.
Backup Autom√°tico: Seguran√ßa contra perda de artefatos.
Prepara√ß√£o para CI/CD: Facilidade na automa√ß√£o de deploys futuros.
Passo a Passo da Integra√ß√£o
1. Criar uma Organiza√ß√£o e um Projeto no Azure DevOps
Acesse o Azure DevOps.
Clique em Criar nova organiza√ß√£o (caso n√£o tenha uma).
Dentro da organiza√ß√£o, clique em Novo projeto.
Defina um nome para o projeto e escolha a visibilidade (Privado ou P√∫blico).
Clique em Criar.
2. Configurar o Reposit√≥rio Git
No projeto criado, v√° para Reposit√≥rios.
Escolha Git como tipo de reposit√≥rio.
Copie a URL do reposit√≥rio remoto para uso posterior.
3. Criar o Azure Data Factory
Acesse o Portal do Azure.
V√° at√© Data Factory e clique em Criar.
Preencha as informa√ß√µes do recurso (Nome, Regi√£o, Grupo de Recursos).
Em Git Configuration, selecione Configure Git later se quiser configurar posteriormente.
Clique em Criar.
4. Configurar a Integra√ß√£o com o Git
Acesse o Azure Data Factory Studio.
No canto superior direito, clique em Manage.
V√° para a aba Git Configuration.
Clique em Configure e insira:
Provider: Azure DevOps Git
Account Name: Nome da organiza√ß√£o no DevOps
Project Name: Nome do projeto criado
Repository Name: Nome do reposit√≥rio
Branch: Defina a branch padr√£o (main ou develop)
Root Folder: /
Clique em Apply.
5. Gerenciando os Artefatos no Git
Todos os pipelines e datasets criados ser√£o salvos no reposit√≥rio.
Para visualizar no Azure DevOps:
Acesse Reposit√≥rios no DevOps e veja os arquivos versionados.
Cada altera√ß√£o ser√° armazenada com um commit.
Para realizar mudan√ßas:
Fa√ßa edi√ß√µes no Data Factory e publique as altera√ß√µes.
No DevOps, crie Pull Requests para revisar e aprovar mudan√ßas antes do merge.
6. Boas Pr√°ticas de Governan√ßa
Utilizar branches dedicadas: feature/nova_funcionalidade, hotfix/corre√ß√£o.
Implementar revis√µes de c√≥digo com Pull Requests.
Manter padr√µes de nomenclatura para pipelines e datasets.
Automatizar deploys futuros com CI/CD utilizando Azure Pipelines.
Conclus√£o
A integra√ß√£o do Azure Data Factory com o Azure DevOps proporciona maior governan√ßa, versionamento e controle sobre os pipelines de dados. Essa estrutura possibilita padronizar ambientes de desenvolvimento e criar uma esteira eficiente para automa√ß√£o futura de deploys no Azure.

